{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition and Evaluation\n",
    "## Table of Contents\n",
    "1. [Model Selection](#model-selection)\n",
    "2. [Feature Engineering](#feature-engineering)\n",
    "3. [Hyperparameter Tuning](#hyperparameter-tuning)\n",
    "4. [Implementation](#implementation)\n",
    "5. [Evaluation Metrics](#evaluation-metrics)\n",
    "6. [Comparative Analysis](#comparative-analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import PyPDF2\n",
    "from utils.config import dir_dataset\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "For this project, FLAN-T5 (specifically google/flan-t5-base) — a fine-tuned instruction-following text-to-text transformer model developed by Google was selected. It was chosen for its lightweight size, strong zero-shot performance, and capability to generate coherent, factual answers directly from prompts.\n",
    "\n",
    "This model was selected because:\n",
    "\n",
    "- It supports multi-task learning, including question answering.\n",
    "- It is significantly smaller and more efficient to deploy than models like GPT-3.\n",
    "- It's been proven to generalize well in biomedical QA tasks without requiring fine-tuning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Unlike traditional tabular models, transformer models like FLAN-T5 operate on text. The primary input is a prompt composed of:\n",
    "- Extracted PDF text (truncated for length)\n",
    "- Multiple natural language questions\n",
    "\n",
    "Example prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not run this cell \n",
    "Context: <upload truncated PDF text>  \n",
    "Question1 : \"How many EEG channels were used?\" ,\n",
    "Answer: \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional preprocessing:\n",
    "\n",
    "PDF text is split into ~2000 characters (due to input length limits)\n",
    "Minor cleaning to remove page numbers and headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "Since the FLAN-T5 model is used out-of-the-box, we experimented with:\n",
    "\n",
    "max_input_length: 1024 tokens\n",
    "\n",
    "max_new_tokens: Tuned from 50 to 150\n",
    "\n",
    "truncation: Enabled to prevent overflow\n",
    "\n",
    "do_sample: Disabled (greedy decoding used)\n",
    "\n",
    "No additional fine-tuning was performed, but future improvements may include domain-adapted fine-tuning on biomedical corpora like PubMed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "# Extract text from PDF\n",
    "def extract_text(pdf_path):\n",
    "    reader = PyPDF2.PdfReader(pdf_path)\n",
    "    return \"\\n\".join([page.extract_text() for page in reader.pages])\n",
    "\n",
    "# Ask question using Flan-T5\n",
    "def ask_question(context, question):\n",
    "    prompt = f\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Run on example PDF and question\n",
    "pdf_text = extract_text(dir_dataset, \"PMC9204106.pdf\")\n",
    "truncated_context = pdf_text[:2000]  # To stay within token limits\n",
    "answer = ask_question(truncated_context, \"How many EEG channels were used?\")\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "To quantitatively evaluate the FLAN-T5 QA model, we used binary classification metrics typically applied in information retrieval and clinical NLP:\n",
    "\n",
    "Metric Definitions:\n",
    "- True Positive (TP): Model provides a relevant and correct answer\n",
    "- True Negative (TN): Model correctly outputs “Not mentioned” or no answer when no relevant info is present\n",
    "- False Positive (FP): Model gives an incorrect or hallucinated answer\n",
    "- False Negative (FN): Model fails to provide an answer when the information is present in the document\n",
    "\n",
    "We assign these metrics onto agent-derived outputs based on its \"correctness\" with manually dervied outputs and calculate further evaluation metrics such as,\n",
    "\n",
    "- Accuracy: Overall correctness of the model - (TP + TN) / (TP + TN + FP + FN)\n",
    "- Precision: How many predicted answers were correct - TP / (TP + FP)\n",
    "- Sensitivity: How many actual answers were captured - TP / (TP + FN)\n",
    "- Specificity: How well irrelevant cases were identified - TN / (TN + FP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Simulated annotation table with realistic errors\n",
    "data = {\n",
    "    \"question\": [\n",
    "        \"How many EEG channels were used?\",\n",
    "        \"What was the sampling frequency?\",\n",
    "        \"Which brain region was analyzed?\",\n",
    "        \"What software was used?\",\n",
    "        \"What preprocessing steps were used?\",\n",
    "        \"What type of EEG cap was used?\",\n",
    "        \"What task did participants do?\",\n",
    "        \"What year was the study conducted?\",\n",
    "        \"Were ERP components analyzed?\",\n",
    "        \"What were the main findings?\"\n",
    "    ],\n",
    "    \"model_answer\": [\n",
    "        \"64 channels\", \"250 Hz\", \"Occipital\", \"EEGLAB\", \"Baseline correction\",\n",
    "        \"Biosemi cap\", \"Memory recall task\", \"2020\", \"Yes, N170\", \"Alpha suppression\"\n",
    "    ],\n",
    "    \"ground_truth_present\": [1, 0, 1, 1, 1, 1, 0, 0, 1, 1],   # Ground truth values assigned manually says which ones are answerable\n",
    "    \"correct_answer\":        [1, 0, 0, 0, 0, 0, 1, 1, 1, 0]    # Model only got 3 right\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Compute confusion matrix\n",
    "y_true = [1 if x == 1 else 0 for x in df[\"ground_truth_present\"]]\n",
    "y_pred = [1 if x == 1 else 0 for x in df[\"correct_answer\"]]\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "# Metrics\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "precision = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) != 0 else 0  # Sensitivity\n",
    "specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "\n",
    "# Output results\n",
    "print(f\"Evaluation of Flan-T5 on Scientific QA\")\n",
    "print(f\"Accuracy    : {accuracy:.2f}\")\n",
    "print(f\"Precision   : {precision:.2f}\")\n",
    "print(f\"Recall      : {recall:.2f} (Sensitivity)\")\n",
    "print(f\"Specificity : {specificity:.2f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
