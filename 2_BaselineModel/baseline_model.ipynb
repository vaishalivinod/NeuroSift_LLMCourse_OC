{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model\n",
    "\n",
    "## Overview \n",
    "The goal of this notebook is to define a simple baseline model for NeuroSift: an AI agent that extracts scientific methods information (e.g., number of EEG channels, programming tools used) from neuroscience papers. This baseline will help measure improvements made by more advanced models like Flan-T5.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Model Choice](#model-choice)\n",
    "2. [Feature Selection](#feature-selection)\n",
    "3. [Implementation](#implementation)\n",
    "4. [Evaluation](#evaluation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import PyPDF2\n",
    "import streamlit as st\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import tempfile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Choice\n",
    "\n",
    "A simple TF-IDF + cosine similarity model was chosen as baseline. For each question (e.g., \"How many EEG channels were used?\"), the model finds the sentence in the PDF that is most similar to the question.\n",
    "\n",
    "This model doesn't require training and can be implemented easily using scikit-learn. It establishes a performance floor based on keyword overlap and semantic similarity, against which more powerful models like FLAN-T5 or SciFive can be evaluated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "Input Features: Sentences extracted from PDF documents.\n",
    "\n",
    "Target Query: A user question (e.g., “Which software was used?”).\n",
    "\n",
    "Representation: TF-IDF vectors for both question and sentences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "[Implement your baseline model here.]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.config import dir_dataset\n",
    "\n",
    "def extract_text(pdf_file):\n",
    "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "    return \"\\n\".join(page.extract_text() for page in pdf_reader.pages)\n",
    "\n",
    "def get_best_match(text, question):\n",
    "    sentences = [s.strip() for s in text.split(\".\") if len(s.strip()) > 10]\n",
    "    corpus = sentences + [question]\n",
    "    vectorizer = TfidfVectorizer().fit(corpus)\n",
    "    tfidf_matrix = vectorizer.transform(corpus)\n",
    "    sims = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])\n",
    "    best_index = sims[0].argmax()\n",
    "    return sentences[best_index], sims[0][best_index]\n",
    "\n",
    "# Example usage:\n",
    "with open(\"PMC9204106.pdf\", \"rb\") as f: \n",
    "    paper_text = extract_text(f)\n",
    "\n",
    "question = \"How many EEG channels were used?\"\n",
    "answer, score = get_best_match(paper_text, question)\n",
    "\n",
    "print(\"Question:\", question)\n",
    "print(\"Best Match:\", answer)\n",
    "print(\"Similarity Score:\", score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "[Clearly state what metrics you will use to evaluate the model's performance. These metrics will serve as a starting point for evaluating more complex models later on.]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the baseline model\n",
    "# Example for a classification problem\n",
    "# y_pred = model.predict(X_test)\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# For a regression problem, you might use:\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Your evaluation code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
